{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6dc0ebeb",
   "metadata": {},
   "source": [
    "## –í—ã–±–æ—Ä —Ñ—Ä–µ–π–º–≤–æ—Ä–∫–∞\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd91304",
   "metadata": {},
   "source": [
    "–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –Ω–æ—É—Ç–±—É–∫, —á—Ç–æ–±—ã –ø–æ —à–∞–≥–∞–º –∑–∞–ø—É—Å–∫–∞—Ç—å –∑–∞–≥—Ä—É–∑–∫—É –º–æ–∫–æ–≤, –≤—ã–±–æ—Ä —Ñ—Ä–µ–π–º–≤–æ—Ä–∫–∞ –∏ –∫–æ–¥–æ–≥–µ–Ω–µ—Ä–∞—Ü–∏—é."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3029806a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f75451b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–ö–ª—é—á–∏ data: ['summary', 'priority', 'steps', 'example_pipeline_snippet', 'frameworks_recommended', 'rationale', 'estimated_complexity', 'confidence']\n",
      "–ö–ª—é—á–∏ metadata: ['dataset_id', 'name', 'version', 'version_label', 'description', 'citation', 'paper_url', 'paper_reference', 'creator', 'contributor', 'collection_date', 'upload_date', 'language', 'licence', 'url', 'original_data_url', 'minio_url', 'format', 'file_id', 'default_target_attribute', 'ignore_attribute', 'row_id_attribute', 'num_rows', 'num_features', 'num_classes', 'num_missing_values', 'quality', 'dataset_type', 'tags', 'status', 'visibility', 'extra_info', 'local_path']\n",
      "–§—Ä–µ–π–º–≤–æ—Ä–∫–∏: ['AutoGluon', 'H2O AutoML', 'LightAutoML']\n"
     ]
    }
   ],
   "source": [
    "from mas_automl.code_agent.load_mocks import load_mock_inputs\n",
    "\n",
    "DATA_ANALYZE, METADATA, REGISTRY, FINAL_DATA = load_mock_inputs()\n",
    "print(f\"–ö–ª—é—á–∏ data: {list[str](DATA_ANALYZE.keys())}\")\n",
    "print(f\"–ö–ª—é—á–∏ metadata: {list(METADATA.keys())}\")\n",
    "print(f\"–§—Ä–µ–π–º–≤–æ—Ä–∫–∏: {list(REGISTRY.keys())}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8586fa58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "### Loaded Mock Inputs\n",
       "\n",
       "#### Data Analyze Keys\n",
       "summary, priority, steps, example_pipeline_snippet, frameworks_recommended, rationale, estimated_complexity, confidence\n",
       "\n",
       "#### Metadata Keys\n",
       "dataset_id, name, version, version_label, description, citation, paper_url, paper_reference, creator, contributor, collection_date, upload_date, language, licence, url, original_data_url, minio_url, format, file_id, default_target_attribute, ignore_attribute, row_id_attribute, num_rows, num_features, num_classes, num_missing_values, quality, dataset_type, tags, status, visibility, extra_info, local_path\n",
       "\n",
       "#### Registry Frameworks\n",
       "AutoGluon, H2O AutoML, LightAutoML\n",
       "\n",
       "manifest, validation_report, metafeatures, preprocessing_recipe, code_agent_recommendation, run_metadata\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from mas_automl.code_agent.load_mocks import load_mock_inputs\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "DATA_ANALYZE, METADATA, REGISTRY, FINAL_DATA = load_mock_inputs()\n",
    "\n",
    "# Pretty display in notebook\n",
    "display(Markdown(f\"\"\"\n",
    "### Loaded Mock Inputs\n",
    "\n",
    "#### Data Analyze Keys\n",
    "{', '.join(DATA_ANALYZE.keys())}\n",
    "\n",
    "#### Metadata Keys\n",
    "{', '.join(METADATA.keys())}\n",
    "\n",
    "#### Registry Frameworks\n",
    "{', '.join(REGISTRY.keys())}\n",
    "\n",
    "{', '.join(FINAL_DATA.keys())}\n",
    "\"\"\"))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e56cd773",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000 entries, 0 to 999\n",
      "Data columns (total 21 columns):\n",
      " #   Column                  Non-Null Count  Dtype  \n",
      "---  ------                  --------------  -----  \n",
      " 0   checking_status         1000 non-null   object \n",
      " 1   duration                1000 non-null   int64  \n",
      " 2   credit_history          1000 non-null   object \n",
      " 3   purpose                 1000 non-null   object \n",
      " 4   credit_amount           1000 non-null   float64\n",
      " 5   savings_status          1000 non-null   object \n",
      " 6   employment              1000 non-null   object \n",
      " 7   installment_commitment  1000 non-null   int64  \n",
      " 8   personal_status         1000 non-null   object \n",
      " 9   other_parties           1000 non-null   object \n",
      " 10  residence_since         1000 non-null   int64  \n",
      " 11  property_magnitude      1000 non-null   object \n",
      " 12  age                     1000 non-null   int64  \n",
      " 13  other_payment_plans     1000 non-null   object \n",
      " 14  housing                 1000 non-null   object \n",
      " 15  existing_credits        1000 non-null   int64  \n",
      " 16  job                     1000 non-null   object \n",
      " 17  num_dependents          1000 non-null   int64  \n",
      " 18  own_telephone           1000 non-null   object \n",
      " 19  foreign_worker          1000 non-null   object \n",
      " 20  class                   1000 non-null   object \n",
      "dtypes: float64(1), int64(6), object(14)\n",
      "memory usage: 164.2+ KB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None,\n",
       "   checking_status  duration                  credit_history   purpose  \\\n",
       " 0              <0         6  critical/other existing credit  radio/tv   \n",
       " 1        0<=X<200        48                   existing paid  radio/tv   \n",
       " \n",
       "    credit_amount    savings_status employment  installment_commitment  \\\n",
       " 0         1169.0  no known savings        >=7                       4   \n",
       " 1         5951.0              <100     1<=X<4                       2   \n",
       " \n",
       "       personal_status other_parties  ...  property_magnitude age  \\\n",
       " 0         male single          none  ...         real estate  67   \n",
       " 1  female div/dep/mar          none  ...         real estate  22   \n",
       " \n",
       "    other_payment_plans housing existing_credits      job num_dependents  \\\n",
       " 0                 none     own                2  skilled              1   \n",
       " 1                 none     own                1  skilled              1   \n",
       " \n",
       "    own_telephone foreign_worker class  \n",
       " 0            yes            yes  good  \n",
       " 1           none            yes   bad  \n",
       " \n",
       " [2 rows x 21 columns])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DATA_ANALYZE, METADATA, REGISTRY\n",
    "import pandas as pd\n",
    "\n",
    "PATH_TO_CSV = FINAL_DATA[\"manifest\"][\"local_path\"]\n",
    "dataset_df = pd.read_csv(PATH_TO_CSV)\n",
    "\n",
    "dataset_df.info(), dataset_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4deb842e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'api_key': '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'}\n"
     ]
    }
   ],
   "source": [
    "# --- –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–ª–∏–µ–Ω—Ç–∞ ---\n",
    "from mas_automl.code_agent.openai_wraper import LLMClient, LLMConfig\n",
    "\n",
    "client = LLMClient(LLMConfig())\n",
    "\n",
    "# --- –í—ã–∑–æ–≤ –≤—ã–±–æ—Ä–∞ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫–∞ ---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "023583c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Asdasfa! How can I assist you today?'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.chat('say asdasfa')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8abdb71e",
   "metadata": {},
   "source": [
    "## –í—ã–±–æ—Ä —Ñ—Ä–µ–π–º–≤–æ—Ä–∫–∞"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "458a62a8",
   "metadata": {},
   "source": [
    "## –ò—Ç–µ—Ä–∞—Ç–∏–≤–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –∏ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–æ–¥–∞\n",
    "\n",
    "–ù–∏–∂–µ –ø–æ–∫–∞–∑–∞–Ω –ø—Ä–æ—Ü–µ—Å—Å –∏—Ç–µ—Ä–∞—Ç–∏–≤–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∫–æ–¥–∞ —Å —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ–º —á–µ—Ä–µ–∑ execnet_gateway\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3e262490",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV –ø—É—Ç—å: C:\\Users\\User1\\Desktop\\ITMO_bootcamp\\data\\datasets\\openml_31_credit-g.csv\n",
      "–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è –ø—Ä–µ–¥–∏–∫—Ç–æ–≤: C:\\Users\\User1\\Desktop\\ITMO_bootcamp\\data\\datasets\\predictions\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# –ò—Ç–µ—Ä–∞—Ç–∏–≤–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –∏ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–æ–¥–∞\n",
    "from mas_automl.code_agent.base_pipeline import generate_code, evaluate_code\n",
    "from mas_automl.code_agent.execnet_gateway import PythonSandboxClient\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "CSV_PATH = FINAL_DATA[\"manifest\"][\"local_path\"]\n",
    "OUTPUT_DIR = str(Path(CSV_PATH).parent / \"predictions\")\n",
    "Path(OUTPUT_DIR).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "framework = \"scikit-learn\"\n",
    "max_iterations = 3\n",
    "feedback = \"\"\n",
    "\n",
    "print(f\"CSV –ø—É—Ç—å: {CSV_PATH}\")\n",
    "print(f\"–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è –ø—Ä–µ–¥–∏–∫—Ç–æ–≤: {OUTPUT_DIR}\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1d8881d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "–ò—Ç–µ—Ä–∞—Ü–∏—è 1/3\n",
      "============================================================\n",
      "\n",
      "–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∫–æ–¥–∞...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-14 15:23:35,785 [INFO] Creating new execnet gateway with python=c:\\Users\\User1\\Desktop\\ITMO_bootcamp\\.venv\\Scripts\\python.exe\n",
      "2025-11-14 15:23:35,859 [INFO] Gateway and channel created.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∫–æ–¥:\n",
      "import pandas as pd\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.compose import ColumnTransformer\n",
      "from sklearn.pipeline import Pipeline\n",
      "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
      "from sklearn.model_selection import train_test_split\n",
      "\n",
      "def train_model(train_df: pd.DataFrame, test_df: pd.DataFrame, label: str):\n",
      "    numeric_features = ['duration', 'credit_amount', 'installment_commitment', 'residence_since', 'age', 'existing_credits', 'num_dependents']\n",
      "    categorical_features = ['checking_status', 'credit_history', 'purpose', 'savings_status', 'employment', 'personal_status', 'other_parties', 'property_magnitude', 'other_payment_plans', 'housing', 'job', 'own_telephone', 'foreign_worker']\n",
      "    \n",
      "    X_train = train_df.drop(columns=[label])\n",
      "    y_train = train_df[label]\n",
      "    \n",
      "    preprocessor = ColumnTransformer(\n",
      "        transformers=[\n",
      "            ('num', StandardScaler(), numeric_features),\n",
      "            ('cat', OneHotEncoder(), categorical_features)\n",
      "        ]\n",
      "    )\n",
      "    \n",
      "    model = Pipeline(steps=[\n",
      "        ('preprocessor', preprocessor),\n",
      "        ('classifier', RandomForestClassifier())\n",
      "    ])\n",
      "    \n",
      "    model.fit(X_train, y_train)\n",
      "    \n",
      "    X_test = test_df.drop(columns=[label])\n",
      "    predictions = model.predict(X_test)\n",
      "    \n",
      "    return model\n",
      "\n",
      "–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–æ–¥–∞...\n",
      "1 - start testing\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'result' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Desktop\\ITMO_bootcamp\\src\\mas_automl\\code_agent\\base_pipeline.py:257\u001b[39m, in \u001b[36mevaluate_code\u001b[39m\u001b[34m(code, framework, csv_path, output_dir, iteration, final_data)\u001b[39m\n\u001b[32m    256\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m257\u001b[39m     stdout = \u001b[43mresult\u001b[49m.stdout\n\u001b[32m    258\u001b[39m     \u001b[38;5;66;03m# –ò—â–µ–º –º–∞—Ä–∫–µ—Ä—ã RESULT_START –∏ RESULT_END\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'result' is not defined",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# –¢–µ—Å—Ç–∏—Ä—É–µ–º –∫–æ–¥\u001b[39;00m\n\u001b[32m     12\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–æ–¥–∞...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m tests_passed, test_feedback, predict_path = \u001b[43mevaluate_code\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframework\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCSV_PATH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mOUTPUT_DIR\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miteration\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mFINAL_DATA\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m–¢–µ—Å—Ç—ã –ø—Ä–æ–π–¥–µ–Ω—ã: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtests_passed\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     18\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m–û–±—Ä–∞—Ç–Ω–∞—è —Å–≤—è–∑—å: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_feedback\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Desktop\\ITMO_bootcamp\\src\\mas_automl\\code_agent\\base_pipeline.py:273\u001b[39m, in \u001b[36mevaluate_code\u001b[39m\u001b[34m(code, framework, csv_path, output_dir, iteration, final_data)\u001b[39m\n\u001b[32m    271\u001b[39m             result_dict = {\u001b[33m\"\u001b[39m\u001b[33mok\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mmessage\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33m–ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤ stdout\u001b[39m\u001b[33m\"\u001b[39m}\n\u001b[32m    272\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m--> \u001b[39m\u001b[32m273\u001b[39m     feedback = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mStdout: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mresult\u001b[49m.stdout\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mStderr: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult.stderr\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    274\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, feedback, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    275\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m4 - end gateway\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'result' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "for iteration in range(1, max_iterations + 1):\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"–ò—Ç–µ—Ä–∞—Ü–∏—è {iteration}/{max_iterations}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –∫–æ–¥\n",
    "    print(\"–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∫–æ–¥–∞...\")\n",
    "    code = generate_code(framework, client, iteration, feedback, FINAL_DATA)\n",
    "    print(f\"–°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∫–æ–¥:\\n{code}\\n\")\n",
    "    \n",
    "    # –¢–µ—Å—Ç–∏—Ä—É–µ–º –∫–æ–¥\n",
    "    print(\"–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–æ–¥–∞...\")\n",
    "    tests_passed, test_feedback, predict_path = evaluate_code(\n",
    "        code, framework, CSV_PATH, OUTPUT_DIR, iteration, FINAL_DATA\n",
    "    )\n",
    "    \n",
    "    print(f\"–¢–µ—Å—Ç—ã –ø—Ä–æ–π–¥–µ–Ω—ã: {tests_passed}\")\n",
    "    print(f\"–û–±—Ä–∞—Ç–Ω–∞—è —Å–≤—è–∑—å: {test_feedback}\")\n",
    "    if predict_path:\n",
    "        print(f\"–ü—É—Ç—å –∫ –ø—Ä–µ–¥–∏–∫—Ç–∞–º: {predict_path}\")\n",
    "        # –ó–∞–≥—Ä—É–∂–∞–µ–º –∏ –ø–æ–∫–∞–∑—ã–≤–∞–µ–º –ø—Ä–µ–¥–∏–∫—Ç—ã\n",
    "        try:\n",
    "            pred_df = pd.read_csv(predict_path)\n",
    "            print(f\"\\n–ü–µ—Ä–≤—ã–µ 5 —Å—Ç—Ä–æ–∫ –ø—Ä–µ–¥–∏–∫—Ç–æ–≤:\")\n",
    "            print(pred_df.head())\n",
    "        except Exception as e:\n",
    "            print(f\"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –ø—Ä–µ–¥–∏–∫—Ç–æ–≤: {e}\")\n",
    "    \n",
    "    print(\"\\n\")\n",
    "    \n",
    "    if tests_passed:\n",
    "        print(\"‚úÖ –í—Å–µ —Ç–µ—Å—Ç—ã –ø—Ä–æ–π–¥–µ–Ω—ã! –û—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –∏—Ç–µ—Ä–∞—Ü–∏–∏.\")\n",
    "        break\n",
    "    else:\n",
    "        feedback = test_feedback\n",
    "        print(\"‚ùå –¢–µ—Å—Ç—ã –Ω–µ –ø—Ä–æ–π–¥–µ–Ω—ã. –ü–µ—Ä–µ—Ö–æ–¥–∏–º –∫ —Å–ª–µ–¥—É—é—â–µ–π –∏—Ç–µ—Ä–∞—Ü–∏–∏ —Å —É—á–µ—Ç–æ–º –æ–±—Ä–∞—Ç–Ω–æ–π —Å–≤—è–∑–∏.\\n\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"–ò—Ç–æ–≥–æ–≤—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç:\")\n",
    "print(f\"–ò—Ç–µ—Ä–∞—Ü–∏–π –≤—ã–ø–æ–ª–Ω–µ–Ω–æ: {iteration}\")\n",
    "print(f\"–¢–µ—Å—Ç—ã –ø—Ä–æ–π–¥–µ–Ω—ã: {tests_passed}\")\n",
    "print(f\"–ü—É—Ç—å –∫ –ø—Ä–µ–¥–∏–∫—Ç–∞–º: {predict_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77cc4c40",
   "metadata": {},
   "source": [
    "## –ü—Ä–æ—Å–º–æ—Ç—Ä —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã—Ö –ø—Ä–µ–¥–∏–∫—Ç–æ–≤\n",
    "\n",
    "–ï—Å–ª–∏ –ø—Ä–µ–¥–∏–∫—Ç—ã –±—ã–ª–∏ —É—Å–ø–µ—à–Ω–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã, –º–æ–∂–Ω–æ –∑–∞–≥—Ä—É–∑–∏—Ç—å –∏ –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –∏—Ö\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d1cb4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ü—Ä–æ—Å–º–æ—Ç—Ä —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã—Ö –ø—Ä–µ–¥–∏–∫—Ç–æ–≤\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "if predict_path and Path(predict_path).exists():\n",
    "    pred_df = pd.read_csv(predict_path)\n",
    "    print(f\"–ó–∞–≥—Ä—É–∂–µ–Ω–æ –ø—Ä–µ–¥–∏–∫—Ç–æ–≤: {len(pred_df)}\")\n",
    "    print(f\"\\n–ü–µ—Ä–≤—ã–µ 10 —Å—Ç—Ä–æ–∫:\")\n",
    "    print(pred_df.head(10))\n",
    "    print(f\"\\n–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:\")\n",
    "    print(pred_df.describe())\n",
    "    \n",
    "    # –ü—Ä–æ—Å—Ç–∞—è –º–µ—Ç—Ä–∏–∫–∞ —Ç–æ—á–Ω–æ—Å—Ç–∏\n",
    "    if 'true_label' in pred_df.columns and 'prediction' in pred_df.columns:\n",
    "        from sklearn.metrics import accuracy_score\n",
    "        accuracy = accuracy_score(pred_df['true_label'], pred_df['prediction'])\n",
    "        print(f\"\\n–¢–æ—á–Ω–æ—Å—Ç—å (accuracy): {accuracy:.4f}\")\n",
    "else:\n",
    "    print(\"–ü—Ä–µ–¥–∏–∫—Ç—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã. –ó–∞–ø—É—Å—Ç–∏—Ç–µ –ø—Ä–µ–¥—ã–¥—É—â—É—é —è—á–µ–π–∫—É –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2916ba0",
   "metadata": {},
   "source": [
    "# –°–¢–ê–†–¨–ï"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c02b4ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "import sys\n",
    "from dataclasses import dataclass\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, Tuple\n",
    "\n",
    "if __package__ is None or __package__ == \"\":\n",
    "    from mas_automl.code_agent.load_mocks import load_mock_inputs  # type: ignore\n",
    "    from mas_automl.code_agent.openai_wraper import LLMClient, LLMConfig  # type: ignore\n",
    "    from mas_automl.code_agent.execnet_gateway import PythonSandboxClient, SandboxResult  # type: ignore\n",
    "else:\n",
    "    from .load_mocks import load_mock_inputs\n",
    "    from .openai_wraper import LLMClient, LLMConfig\n",
    "    from .execnet_gateway import PythonSandboxClient, SandboxResult\n",
    "\n",
    "DEFAULT_MAX_ITERATIONS = 3\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class PipelineResult:\n",
    "    framework: str\n",
    "    reason: str\n",
    "    code: str\n",
    "    tests_passed: bool\n",
    "    iterations: int\n",
    "    feedback: str\n",
    "    predict_path: str | None = None\n",
    "\n",
    "\n",
    "def evaluate_code(\n",
    "    code: str,\n",
    "    framework: str,\n",
    "    csv_path: str,\n",
    "    CSV_PATH_TO_PREDICT: str,\n",
    "    output_dir: str,\n",
    "    iteration: int,\n",
    "    final_data: Dict[str, Any] | None = None,\n",
    ") -> Tuple[bool, str, str | None]:\n",
    "    \"\"\"\n",
    "    –í—ã–ø–æ–ª–Ω—è–µ—Ç –∫–æ–¥ –≤ –ø–µ—Å–æ—á–Ω–∏—Ü–µ, —Ç–µ—Å—Ç–∏—Ä—É–µ—Ç —Ñ—É–Ω–∫—Ü–∏—é train_model –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –ø—Ä–µ–¥–∏–∫—Ç—ã.\n",
    "    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç (tests_passed, feedback, predict_path).\n",
    "    \"\"\"\n",
    "    sandbox = PythonSandboxClient.get()\n",
    "    \n",
    "    # –ü–æ–ª—É—á–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –¥–∞—Ç–∞—Å–µ—Ç–µ\n",
    "    target_column = \"class\"\n",
    "    if final_data:\n",
    "        target_column = final_data.get(\"manifest\", {}).get(\"target_column\", \"class\")\n",
    "    print(\"1 - start testing\")\n",
    "    # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –∫–æ–¥ –¥–ª—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è\n",
    "    test_code = f\"\"\"\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import json\n",
    "from datetime import datetime\n",
    "from automl import AutoML\n",
    "\n",
    "\n",
    "# –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ\n",
    "df = pd.read_csv(r'{csv_path}')\n",
    "print(f\"–ó–∞–≥—Ä—É–∂–µ–Ω–æ —Å—Ç—Ä–æ–∫: {{len(df)}}\")\n",
    "\n",
    "# –ì–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∫–æ–¥ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è\n",
    "{code}\n",
    "\n",
    "# –í—ã–ø–æ–ª–Ω—è–µ–º —Ñ—É–Ω–∫—Ü–∏—é\n",
    "try:\n",
    "    df['pred'] = np.random.rand(len(df))\n",
    "    df.to_csv(r'{CSV_PATH_TO_PREDICT}', index=False)\n",
    "    # print(f\"–ü—Ä–µ–¥–∏–∫—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {CSV_PATH_TO_PREDICT}\")\n",
    "    \n",
    "    # –ü—Ä–æ—Å—Ç—ã–µ –ø—Ä–æ–≤–µ—Ä–∫–∏\n",
    "    errors = []\n",
    "   \n",
    "    if errors and any(errors):\n",
    "        raise ValueError(\"; \".join([e for e in errors if e]))\n",
    "    \n",
    "    result = {{\"ok\": True, \"predict_path\": \"mock\", \"message\": \"–í—Å–µ –ø—Ä–æ–≤–µ—Ä–∫–∏ –ø—Ä–æ–π–¥–µ–Ω—ã\"}}\n",
    "    \n",
    "except Exception as e:\n",
    "    import traceback\n",
    "    result = {{\"ok\": False, \"predict_path\": None, \"message\": str(e), \"traceback\": traceback.format_exc()}}\n",
    "\n",
    "# –í—ã–≤–æ–¥–∏–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤ —Ñ–æ—Ä–º–∞—Ç–µ JSON –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞\n",
    "print(\"RESULT_START\")\n",
    "print(json.dumps(result, ensure_ascii=False))\n",
    "print(\"RESULT_END\")\n",
    "\"\"\"\n",
    "    \n",
    "    # –í—ã–ø–æ–ª–Ω—è–µ–º –∫–æ–¥ –≤ –ø–µ—Å–æ—á–Ω–∏—Ü–µ\n",
    "    result = sandbox.run(test_code)\n",
    "    print(\"2 - end testing\")\n",
    "  \n",
    "    if not result.ok:\n",
    "        feedback = f\"–û—à–∏–±–∫–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è: {result.stderr}\\n{result.stdout}\"\n",
    "        return False, feedback, None\n",
    "    print(\"3 - try gateway\")\n",
    "\n",
    "    # –ü–∞—Ä—Å–∏–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç –∏–∑ stdout\n",
    "    try:\n",
    "        stdout = result.stdout\n",
    "        # –ò—â–µ–º –º–∞—Ä–∫–µ—Ä—ã RESULT_START –∏ RESULT_END\n",
    "        if \"RESULT_START\" in stdout and \"RESULT_END\" in stdout:\n",
    "            start_idx = stdout.find(\"RESULT_START\") + len(\"RESULT_START\")\n",
    "            end_idx = stdout.find(\"RESULT_END\")\n",
    "            result_json = stdout[start_idx:end_idx].strip()\n",
    "            result_dict = json.loads(result_json)\n",
    "        else:\n",
    "            # Fallback: –ø—ã—Ç–∞–µ–º—Å—è –Ω–∞–π—Ç–∏ JSON –≤ stdout\n",
    "            import re\n",
    "            json_match = re.search(r'\\{[^{}]*\"ok\"[^{}]*\\}', stdout)\n",
    "            if json_match:\n",
    "                result_dict = json.loads(json_match.group())\n",
    "            else:\n",
    "                result_dict = {\"ok\": False, \"message\": \"–ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤ stdout\"}\n",
    "    except Exception as e:\n",
    "        feedback = f\"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞: {e}\\nStdout: {result.stdout}\\nStderr: {result.stderr}\"\n",
    "        return False, feedback, None\n",
    "    print(\"4 - end gateway\")\n",
    "\n",
    "    if result_dict.get(\"ok\", False):\n",
    "        predict_path = result_dict.get(\"predict_path\")\n",
    "        message = result_dict.get(\"message\", \"–ü—Ä–æ–≤–µ—Ä–∫–∏ –ø—Ä–æ–π–¥–µ–Ω—ã\")\n",
    "        return True, message, predict_path\n",
    "    else:\n",
    "        error_msg = result_dict.get(\"message\", \"–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –æ—à–∏–±–∫–∞\")\n",
    "        traceback_info = result_dict.get(\"traceback\", \"\")\n",
    "        feedback = f\"–¢–µ—Å—Ç—ã –Ω–µ –ø—Ä–æ–π–¥–µ–Ω—ã: {error_msg}\\n{traceback_info}\"\n",
    "        return False, feedback, None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0a49ced5",
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV_PATH_TO_PREDICT = 'C:\\\\Users\\\\User1\\\\Desktop\\\\ITMO_bootcamp\\\\data\\\\datasets\\\\TEST\\\\test1__.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1ed49a52",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-14 19:04:38,553 [INFO] Creating new execnet gateway with python=c:\\Users\\User1\\Desktop\\ITMO_bootcamp\\.venv\\Scripts\\python.exe\n",
      "2025-11-14 19:04:38,645 [INFO] Gateway and channel created.\n",
      "2025-11-14 19:04:38,646 [DEBUG] Sending code to sandbox (attempt 1):\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from pathlib import Path\n",
      "import json\n",
      "from datetime import datetime\n",
      "from automl import AutoML\n",
      "\n",
      "\n",
      "# –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ\n",
      "df = pd.read_csv(r'C:\\Users\\User1\\Desktop\\ITMO_bootcamp\\data\\datasets\\openml_31_credit-g.csv')\n",
      "print(f\"–ó–∞–≥—Ä—É–∂–µ–Ω–æ —Å—Ç—Ä–æ–∫: {len(df)}\")\n",
      "\n",
      "# –ì–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∫–æ–¥ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è\n",
      "print(5)\n",
      "\n",
      "# –í—ã–ø–æ–ª–Ω—è–µ–º —Ñ—É–Ω–∫—Ü–∏—é\n",
      "try:\n",
      "    df['pred'] = np.random.rand(len(df))\n",
      "    df.to_csv(r'C:\\Users\\User1\\Desktop\\ITMO_bootcamp\\data\\datasets\\TEST\\test1__.csv', index=False)\n",
      "    # print(f\"–ü—Ä–µ–¥–∏–∫—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: C:\\Users\\User1\\Desktop\\ITMO_bootcamp\\data\\datasets\\TEST\\test1__.csv\")\n",
      "\n",
      "    # –ü—Ä–æ—Å—Ç—ã–µ –ø—Ä–æ–≤–µ—Ä–∫–∏\n",
      "    errors = []\n",
      "\n",
      "    if errors and any(errors):\n",
      "        raise ValueError(\"; \".join([e for e in errors if e]))\n",
      "\n",
      "    result = {\"ok\": True, \"predict_path\": \"mock\", \"message\": \"–í—Å–µ –ø—Ä–æ–≤–µ—Ä–∫–∏ –ø—Ä–æ–π–¥–µ–Ω—ã\"}\n",
      "\n",
      "except Exception as e:\n",
      "    import traceback\n",
      "    result = {\"ok\": False, \"predict_path\": None, \"message\": str(e), \"traceback\": traceback.format_exc()}\n",
      "\n",
      "# –í—ã–≤–æ–¥–∏–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤ —Ñ–æ—Ä–º–∞...[truncated]\n",
      "2025-11-14 19:04:38,647 [DEBUG] Code sent, waiting for result...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–æ–¥–∞...\n",
      "1 - start testing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-14 19:04:44,692 [DEBUG] Received result from sandbox (attempt 1).\n",
      "2025-11-14 19:04:44,693 [INFO] [sandbox stdout]\n",
      "–ó–∞–≥—Ä—É–∂–µ–Ω–æ —Å—Ç—Ä–æ–∫: 1000\n",
      "5\n",
      "RESULT_START\n",
      "{\"ok\": true, \"predict_path\": \"mock\", \"message\": \"–í—Å–µ –ø—Ä–æ–≤–µ—Ä–∫–∏ –ø—Ä–æ–π–¥–µ–Ω—ã\"}\n",
      "RESULT_END\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 - end testing\n",
      "3 - try gateway\n",
      "4 - end gateway\n",
      "–¢–µ—Å—Ç—ã –ø—Ä–æ–π–¥–µ–Ω—ã: True\n",
      "–û–±—Ä–∞—Ç–Ω–∞—è —Å–≤—è–∑—å: –í—Å–µ –ø—Ä–æ–≤–µ—Ä–∫–∏ –ø—Ä–æ–π–¥–µ–Ω—ã\n",
      "–ü—É—Ç—å –∫ –ø—Ä–µ–¥–∏–∫—Ç–∞–º: C:\\Users\\User1\\Desktop\\ITMO_bootcamp\\data\\datasets\\TEST\\test1__.csv\n",
      "\n",
      "–ü–µ—Ä–≤—ã–µ 5 —Å—Ç—Ä–æ–∫ –ø—Ä–µ–¥–∏–∫—Ç–æ–≤:\n",
      "  checking_status  duration                  credit_history  \\\n",
      "0              <0         6  critical/other existing credit   \n",
      "1        0<=X<200        48                   existing paid   \n",
      "2     no checking        12  critical/other existing credit   \n",
      "3              <0        42                   existing paid   \n",
      "4              <0        24              delayed previously   \n",
      "\n",
      "               purpose  credit_amount    savings_status employment  \\\n",
      "0             radio/tv         1169.0  no known savings        >=7   \n",
      "1             radio/tv         5951.0              <100     1<=X<4   \n",
      "2            education         2096.0              <100     4<=X<7   \n",
      "3  furniture/equipment         7882.0              <100     4<=X<7   \n",
      "4              new car         4870.0              <100     1<=X<4   \n",
      "\n",
      "   installment_commitment     personal_status other_parties  ...  age  \\\n",
      "0                       4         male single          none  ...   67   \n",
      "1                       2  female div/dep/mar          none  ...   22   \n",
      "2                       2         male single          none  ...   49   \n",
      "3                       2         male single     guarantor  ...   45   \n",
      "4                       3         male single          none  ...   53   \n",
      "\n",
      "  other_payment_plans   housing existing_credits                 job  \\\n",
      "0                none       own                2             skilled   \n",
      "1                none       own                1             skilled   \n",
      "2                none       own                1  unskilled resident   \n",
      "3                none  for free                1             skilled   \n",
      "4                none  for free                2             skilled   \n",
      "\n",
      "   num_dependents own_telephone  foreign_worker class      pred  \n",
      "0               1           yes             yes  good  0.489676  \n",
      "1               1          none             yes   bad  0.102710  \n",
      "2               2          none             yes  good  0.697033  \n",
      "3               2          none             yes  good  0.620428  \n",
      "4               2          none             yes   bad  0.630522  \n",
      "\n",
      "[5 rows x 22 columns]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print(\"–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∫–æ–¥–∞...\")\n",
    "# code = generate_code(framework, client, iteration, feedback, FINAL_DATA)\n",
    "# print(f\"–°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∫–æ–¥:\\n{code}\\n\")\n",
    "iteration = 1 \n",
    "# –¢–µ—Å—Ç–∏—Ä—É–µ–º –∫–æ–¥\n",
    "print(\"–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–æ–¥–∞...\")\n",
    "tests_passed, test_feedback, predict_path = evaluate_code(\n",
    "    \"print(5)\", framework, CSV_PATH, CSV_PATH_TO_PREDICT, OUTPUT_DIR, iteration, FINAL_DATA\n",
    ")\n",
    "\n",
    "predict_path = CSV_PATH_TO_PREDICT\n",
    "\n",
    "print(f\"–¢–µ—Å—Ç—ã –ø—Ä–æ–π–¥–µ–Ω—ã: {tests_passed}\")\n",
    "print(f\"–û–±—Ä–∞—Ç–Ω–∞—è —Å–≤—è–∑—å: {test_feedback}\")\n",
    "if predict_path:\n",
    "    print(f\"–ü—É—Ç—å –∫ –ø—Ä–µ–¥–∏–∫—Ç–∞–º: {predict_path}\")\n",
    "    # –ó–∞–≥—Ä—É–∂–∞–µ–º –∏ –ø–æ–∫–∞–∑—ã–≤–∞–µ–º –ø—Ä–µ–¥–∏–∫—Ç—ã\n",
    "    try:\n",
    "        pred_df = pd.read_csv(predict_path)\n",
    "        print(f\"\\n–ü–µ—Ä–≤—ã–µ 5 —Å—Ç—Ä–æ–∫ –ø—Ä–µ–¥–∏–∫—Ç–æ–≤:\")\n",
    "        print(pred_df.head())\n",
    "    except Exception as e:\n",
    "        print(f\"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –ø—Ä–µ–¥–∏–∫—Ç–æ–≤: {e}\")\n",
    "\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f59277b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "### Selected Framework\n",
       "**Framework:** H2O AutoML\n",
       "\n",
       "### Reasoning\n",
       "H2O AutoML —è–≤–ª—è–µ—Ç—Å—è –Ω–∞–∏–±–æ–ª–µ–µ —Å—Ç–∞–±–∏–ª—å–Ω—ã–º –∏ —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–º –≤—ã–±–æ—Ä–æ–º –¥–ª—è —Ç–∞–±–ª–∏—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞ (1000 —Å—Ç—Ä–æ–∫, 21 –ø—Ä–∏–∑–Ω–∞–∫–æ–≤) –∏ —Ç–∏–ø–∞ –∑–∞–¥–∞—á–∏ (–±–∏–Ω–∞—Ä–Ω–∞—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è). –û–Ω–æ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç –Ω–∞–¥–µ–∂–Ω—ã–π —Å—Ç–µ–∫–∏–Ω–≥ –º–æ–¥–µ–ª–µ–π (GBM, XGBoost, GLM, DRF, DeepLearning), –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫—É—é –∫—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—é, —Ä–∞–Ω–Ω—é—é –æ—Å—Ç–∞–Ω–æ–≤–∫—É –∏ –ø–æ–¥–¥–µ—Ä–∂–∫—É production-—Å—Ü–µ–Ω–∞—Ä–∏–µ–≤. –ù–µ—Å–º–æ—Ç—Ä—è –Ω–∞ –æ—Ç—Å—É—Ç—Å—Ç–≤–∏–µ –ø–æ–¥–¥–µ—Ä–∂–∫–∏ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ—Å—Ç–∏ –∏ GPU –¥–ª—è –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π, –æ–Ω–æ —Ö–æ—Ä–æ—à–æ –ø–æ–¥—Ö–æ–¥–∏—Ç –¥–ª—è –∑–∞–¥–∞—á –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Å –≤—ã—Å–æ–∫–æ–π –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç—å—é –∏ —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å—é, —á—Ç–æ –æ—Å–æ–±–µ–Ω–Ω–æ –≤–∞–∂–Ω–æ –≤ –∫–æ—Ä–ø–æ—Ä–∞—Ç–∏–≤–Ω—ã—Ö –∏ –ø—Ä–æ–º—ã—à–ª–µ–Ω–Ω—ã—Ö —É—Å–ª–æ–≤–∏—è—Ö. –í –æ—Ç–ª–∏—á–∏–µ –æ—Ç AutoGluon, –∫–æ—Ç–æ—Ä–æ–µ —Ç—Ä–µ–±—É–µ—Ç –±–æ–ª—å—à–µ —Ä–µ—Å—É—Ä—Å–æ–≤ –∏ –º–æ–∂–µ—Ç –±—ã—Ç—å –º–µ–Ω–µ–µ –ø—Ä–µ–¥—Å–∫–∞–∑—É–µ–º–æ –Ω–∞ –Ω–µ–±–æ–ª—å—à–∏—Ö –Ω–∞–±–æ—Ä–∞—Ö –¥–∞–Ω–Ω—ã—Ö, H2O AutoML –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –±–æ–ª–µ–µ –ø—Ä–µ–¥—Å–∫–∞–∑—É–µ–º–æ–µ –ø–æ–≤–µ–¥–µ–Ω–∏–µ –∏ –±–æ–ª–µ–µ —à–∏—Ä–æ–∫—É—é —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å —Å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º–∏ –∏–Ω—Ñ—Ä–∞—Å—Ç—Ä—É–∫—Ç—É—Ä–Ω—ã–º–∏ —Ä–µ—à–µ–Ω–∏—è–º–∏.\n",
       "\n",
       "### Generated Prompt\n",
       "input_variables=['analysis_json', 'frameworks_list', 'metadata_json'] input_types={} partial_variables={} messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='–¢—ã ‚Äî —ç–∫—Å–ø–µ—Ä—Ç –ø–æ AutoML –∏ ML-–∏–Ω–∂–µ–Ω–µ—Ä. –¢–≤–æ—è –∑–∞–¥–∞—á–∞ ‚Äî –≤—ã–±—Ä–∞—Ç—å –Ω–∞–∏–±–æ–ª–µ–µ –ø–æ–¥—Ö–æ–¥—è—â–∏–π AutoML-—Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è —Ç–∞–±–ª–∏—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö. –û—Ü–µ–Ω–∏ –∫–∞—á–µ—Å—Ç–≤–æ –¥–∞–Ω–Ω—ã—Ö, —Ä–∞–∑–º–µ—Ä, —Ç–∏–ø –∑–∞–¥–∞—á–∏, –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –∏ –ø—Ä–µ–¥–ª–æ–∂–∏ –ª—É—á—à–∏–π –≤–∞—Ä–∏–∞–Ω—Ç –∏–∑ —Å–ø–∏—Å–∫–∞.'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['analysis_json', 'frameworks_list', 'metadata_json'], input_types={}, partial_variables={}, template='–í–æ—Ç –∞–Ω–∞–ª–∏–∑ –¥–∞—Ç–∞—Å–µ—Ç–∞ –∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ:\\n\\n### üìä Data Analysis\\n{analysis_json}\\n\\n### üßæ Metadata\\n{metadata_json}\\n\\n### ‚öôÔ∏è –î–æ—Å—Ç—É–ø–Ω—ã–µ AutoML —Ñ—Ä–µ–π–º–≤–æ—Ä–∫–∏\\n{frameworks_list}\\n\\n–ü–æ—è—Å–Ω–∏ —Å–≤–æ–π –≤—ã–±–æ—Ä –∫—Ä–∞—Ç–∫–æ, –Ω–æ —Å–æ–¥–µ—Ä–∂–∞—Ç–µ–ª—å–Ω–æ. –ï—Å–ª–∏ –Ω–µ—Å–∫–æ–ª—å–∫–æ –ø–æ–¥—Ö–æ–¥—è—Ç, –≤—ã–±–µ—Ä–∏ –Ω–∞–∏–±–æ–ª–µ–µ —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –∏ —Å—Ç–∞–±–∏–ª—å–Ω—ã–π –≤–∞—Ä–∏–∞–Ω—Ç. –û—Ç–≤–µ—Ç –≤–µ—Ä–Ω–∏ –≤ —Ñ–æ—Ä–º–∞—Ç–µ JSON:\\n{{\"framework\": \"...\", \"reason\": \"...\"}}'), additional_kwargs={})]\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "# Your existing call (unchanged)\n",
    "from mas_automl.code_agent.base_pipeline import choose_framework\n",
    "\n",
    "framework, reason, prompt = choose_framework(\n",
    "    DATA_ANALYZE, METADATA, REGISTRY, FINAL_DATA, client)\n",
    "\n",
    "# Pretty display in notebook\n",
    "display(Markdown(f\"\"\"\n",
    "### Selected Framework\n",
    "**Framework:** {framework}\n",
    "\n",
    "### Reasoning\n",
    "{reason}\n",
    "\n",
    "### Generated Prompt\n",
    "{prompt}\n",
    "\"\"\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "efb6d4f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.prompts.chat.ChatPromptTemplate'>\n"
     ]
    }
   ],
   "source": [
    "print(type(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e9ce2a",
   "metadata": {},
   "source": [
    "# –ö–æ–¥–æ–≥–µ–Ω–µ—Ä–∞—Ü–∏—è"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b098bf0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import h2o\n",
      "from h2o.automl import H2OAutoML\n",
      "\n",
      "def train_model(train_df, test_df, label):\n",
      "    h2o.init()\n",
      "    train = h2o.H2OFrame(train_df)\n",
      "    test = h2o.H2OFrame(test_df)\n",
      "    train[label] = train[label].asfactor()\n",
      "    test[label] = test[label].asfactor()\n",
      "    \n",
      "    aml = H2OAutoML(max_models=10, seed=1)\n",
      "    aml.train(x=train.columns, y=label, training_frame=train)\n",
      "    \n",
      "    return aml.leader\n",
      "(True, '–ü—Ä–æ–≤–µ—Ä–∫–∏ –ø—Ä–æ–π–¥–µ–Ω—ã.')\n"
     ]
    }
   ],
   "source": [
    "from mas_automl.code_agent.base_pipeline import generate_code, evaluate_code\n",
    "\n",
    "code_sample = generate_code(framework, client, iteration=1, feedback=\"\")\n",
    "print(code_sample)\n",
    "print(evaluate_code(code_sample, framework))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6582026e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.7.2'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sklearn; sklearn.__version__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b58e883d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Usage:   \n",
      "  pip freeze [options]\n",
      "\n",
      "no such option: -m\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f51fe3c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from automl import AutoML"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
