{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6dc0ebeb",
   "metadata": {},
   "source": [
    "## Выбор фреймворка\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd91304",
   "metadata": {},
   "source": [
    "Используйте ноутбук, чтобы по шагам запускать загрузку моков, выбор фреймворка и кодогенерацию."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3029806a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f75451b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ключи data: ['summary', 'priority', 'steps', 'example_pipeline_snippet', 'frameworks_recommended', 'rationale', 'estimated_complexity', 'confidence']\n",
      "Ключи metadata: ['dataset_id', 'name', 'version', 'version_label', 'description', 'citation', 'paper_url', 'paper_reference', 'creator', 'contributor', 'collection_date', 'upload_date', 'language', 'licence', 'url', 'original_data_url', 'minio_url', 'format', 'file_id', 'default_target_attribute', 'ignore_attribute', 'row_id_attribute', 'num_rows', 'num_features', 'num_classes', 'num_missing_values', 'quality', 'dataset_type', 'tags', 'status', 'visibility', 'extra_info', 'local_path']\n",
      "Фреймворки: ['AutoGluon', 'H2O AutoML', 'LightAutoML']\n"
     ]
    }
   ],
   "source": [
    "from mas_automl.code_agent.load_mocks import load_mock_inputs\n",
    "\n",
    "DATA_ANALYZE, METADATA, REGISTRY, FINAL_DATA = load_mock_inputs()\n",
    "print(f\"Ключи data: {list[str](DATA_ANALYZE.keys())}\")\n",
    "print(f\"Ключи metadata: {list(METADATA.keys())}\")\n",
    "print(f\"Фреймворки: {list(REGISTRY.keys())}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8586fa58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "### Loaded Mock Inputs\n",
       "\n",
       "#### Data Analyze Keys\n",
       "summary, priority, steps, example_pipeline_snippet, frameworks_recommended, rationale, estimated_complexity, confidence\n",
       "\n",
       "#### Metadata Keys\n",
       "dataset_id, name, version, version_label, description, citation, paper_url, paper_reference, creator, contributor, collection_date, upload_date, language, licence, url, original_data_url, minio_url, format, file_id, default_target_attribute, ignore_attribute, row_id_attribute, num_rows, num_features, num_classes, num_missing_values, quality, dataset_type, tags, status, visibility, extra_info, local_path\n",
       "\n",
       "#### Registry Frameworks\n",
       "AutoGluon, H2O AutoML, LightAutoML\n",
       "\n",
       "manifest, validation_report, metafeatures, preprocessing_recipe, code_agent_recommendation, run_metadata\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from mas_automl.code_agent.load_mocks import load_mock_inputs\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "DATA_ANALYZE, METADATA, REGISTRY, FINAL_DATA = load_mock_inputs()\n",
    "\n",
    "# Pretty display in notebook\n",
    "display(Markdown(f\"\"\"\n",
    "### Loaded Mock Inputs\n",
    "\n",
    "#### Data Analyze Keys\n",
    "{', '.join(DATA_ANALYZE.keys())}\n",
    "\n",
    "#### Metadata Keys\n",
    "{', '.join(METADATA.keys())}\n",
    "\n",
    "#### Registry Frameworks\n",
    "{', '.join(REGISTRY.keys())}\n",
    "\n",
    "{', '.join(FINAL_DATA.keys())}\n",
    "\"\"\"))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e56cd773",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000 entries, 0 to 999\n",
      "Data columns (total 21 columns):\n",
      " #   Column                  Non-Null Count  Dtype  \n",
      "---  ------                  --------------  -----  \n",
      " 0   checking_status         1000 non-null   object \n",
      " 1   duration                1000 non-null   int64  \n",
      " 2   credit_history          1000 non-null   object \n",
      " 3   purpose                 1000 non-null   object \n",
      " 4   credit_amount           1000 non-null   float64\n",
      " 5   savings_status          1000 non-null   object \n",
      " 6   employment              1000 non-null   object \n",
      " 7   installment_commitment  1000 non-null   int64  \n",
      " 8   personal_status         1000 non-null   object \n",
      " 9   other_parties           1000 non-null   object \n",
      " 10  residence_since         1000 non-null   int64  \n",
      " 11  property_magnitude      1000 non-null   object \n",
      " 12  age                     1000 non-null   int64  \n",
      " 13  other_payment_plans     1000 non-null   object \n",
      " 14  housing                 1000 non-null   object \n",
      " 15  existing_credits        1000 non-null   int64  \n",
      " 16  job                     1000 non-null   object \n",
      " 17  num_dependents          1000 non-null   int64  \n",
      " 18  own_telephone           1000 non-null   object \n",
      " 19  foreign_worker          1000 non-null   object \n",
      " 20  class                   1000 non-null   object \n",
      "dtypes: float64(1), int64(6), object(14)\n",
      "memory usage: 164.2+ KB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None,\n",
       "   checking_status  duration                  credit_history   purpose  \\\n",
       " 0              <0         6  critical/other existing credit  radio/tv   \n",
       " 1        0<=X<200        48                   existing paid  radio/tv   \n",
       " \n",
       "    credit_amount    savings_status employment  installment_commitment  \\\n",
       " 0         1169.0  no known savings        >=7                       4   \n",
       " 1         5951.0              <100     1<=X<4                       2   \n",
       " \n",
       "       personal_status other_parties  ...  property_magnitude age  \\\n",
       " 0         male single          none  ...         real estate  67   \n",
       " 1  female div/dep/mar          none  ...         real estate  22   \n",
       " \n",
       "    other_payment_plans housing existing_credits      job num_dependents  \\\n",
       " 0                 none     own                2  skilled              1   \n",
       " 1                 none     own                1  skilled              1   \n",
       " \n",
       "    own_telephone foreign_worker class  \n",
       " 0            yes            yes  good  \n",
       " 1           none            yes   bad  \n",
       " \n",
       " [2 rows x 21 columns])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DATA_ANALYZE, METADATA, REGISTRY\n",
    "import pandas as pd\n",
    "\n",
    "PATH_TO_CSV = FINAL_DATA[\"manifest\"][\"local_path\"]\n",
    "dataset_df = pd.read_csv(PATH_TO_CSV)\n",
    "\n",
    "dataset_df.info(), dataset_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4deb842e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'api_key': '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'}\n"
     ]
    }
   ],
   "source": [
    "# --- Инициализация клиента ---\n",
    "from mas_automl.code_agent.openai_wraper import LLMClient, LLMConfig\n",
    "\n",
    "client = LLMClient(LLMConfig())\n",
    "\n",
    "# --- Вызов выбора фреймворка ---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "023583c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Yes, I'm here! How can I assist you today?\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.chat('Health-check. Are you alive? short answer')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8abdb71e",
   "metadata": {},
   "source": [
    "## Выбор фреймворка"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "458a62a8",
   "metadata": {},
   "source": [
    "## Итеративная генерация и тестирование кода\n",
    "\n",
    "Ниже показан процесс итеративной генерации кода с тестированием через execnet_gateway\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3e262490",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV путь: C:\\Users\\User1\\Desktop\\ITMO_bootcamp\\data\\datasets\\openml_31_credit-g.csv\n",
      "Директория для предиктов: C:\\Users\\User1\\Desktop\\ITMO_bootcamp\\data\\datasets\\predictions\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Итеративная генерация и тестирование кода\n",
    "from mas_automl.code_agent.base_pipeline import generate_code, evaluate_code\n",
    "from mas_automl.code_agent.execnet_gateway import PythonSandboxClient\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "CSV_PATH = FINAL_DATA[\"manifest\"][\"local_path\"]\n",
    "OUTPUT_DIR = str(Path(CSV_PATH).parent / \"predictions\")\n",
    "Path(OUTPUT_DIR).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "framework = \"scikit-learn\"\n",
    "max_iterations = 3\n",
    "feedback = \"\"\n",
    "\n",
    "print(f\"CSV путь: {CSV_PATH}\")\n",
    "print(f\"Директория для предиктов: {OUTPUT_DIR}\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1d8881d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Итерация 1/3\n",
      "============================================================\n",
      "\n",
      "Генерация кода...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-14 15:23:35,785 [INFO] Creating new execnet gateway with python=c:\\Users\\User1\\Desktop\\ITMO_bootcamp\\.venv\\Scripts\\python.exe\n",
      "2025-11-14 15:23:35,859 [INFO] Gateway and channel created.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Сгенерированный код:\n",
      "import pandas as pd\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.compose import ColumnTransformer\n",
      "from sklearn.pipeline import Pipeline\n",
      "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
      "from sklearn.model_selection import train_test_split\n",
      "\n",
      "def train_model(train_df: pd.DataFrame, test_df: pd.DataFrame, label: str):\n",
      "    numeric_features = ['duration', 'credit_amount', 'installment_commitment', 'residence_since', 'age', 'existing_credits', 'num_dependents']\n",
      "    categorical_features = ['checking_status', 'credit_history', 'purpose', 'savings_status', 'employment', 'personal_status', 'other_parties', 'property_magnitude', 'other_payment_plans', 'housing', 'job', 'own_telephone', 'foreign_worker']\n",
      "    \n",
      "    X_train = train_df.drop(columns=[label])\n",
      "    y_train = train_df[label]\n",
      "    \n",
      "    preprocessor = ColumnTransformer(\n",
      "        transformers=[\n",
      "            ('num', StandardScaler(), numeric_features),\n",
      "            ('cat', OneHotEncoder(), categorical_features)\n",
      "        ]\n",
      "    )\n",
      "    \n",
      "    model = Pipeline(steps=[\n",
      "        ('preprocessor', preprocessor),\n",
      "        ('classifier', RandomForestClassifier())\n",
      "    ])\n",
      "    \n",
      "    model.fit(X_train, y_train)\n",
      "    \n",
      "    X_test = test_df.drop(columns=[label])\n",
      "    predictions = model.predict(X_test)\n",
      "    \n",
      "    return model\n",
      "\n",
      "Тестирование кода...\n",
      "1 - start testing\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'result' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Desktop\\ITMO_bootcamp\\src\\mas_automl\\code_agent\\base_pipeline.py:257\u001b[39m, in \u001b[36mevaluate_code\u001b[39m\u001b[34m(code, framework, csv_path, output_dir, iteration, final_data)\u001b[39m\n\u001b[32m    256\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m257\u001b[39m     stdout = \u001b[43mresult\u001b[49m.stdout\n\u001b[32m    258\u001b[39m     \u001b[38;5;66;03m# Ищем маркеры RESULT_START и RESULT_END\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'result' is not defined",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# Тестируем код\u001b[39;00m\n\u001b[32m     12\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mТестирование кода...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m tests_passed, test_feedback, predict_path = \u001b[43mevaluate_code\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframework\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCSV_PATH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mOUTPUT_DIR\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miteration\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mFINAL_DATA\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mТесты пройдены: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtests_passed\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     18\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mОбратная связь: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_feedback\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Desktop\\ITMO_bootcamp\\src\\mas_automl\\code_agent\\base_pipeline.py:273\u001b[39m, in \u001b[36mevaluate_code\u001b[39m\u001b[34m(code, framework, csv_path, output_dir, iteration, final_data)\u001b[39m\n\u001b[32m    271\u001b[39m             result_dict = {\u001b[33m\"\u001b[39m\u001b[33mok\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mmessage\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mНе удалось найти результат в stdout\u001b[39m\u001b[33m\"\u001b[39m}\n\u001b[32m    272\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m--> \u001b[39m\u001b[32m273\u001b[39m     feedback = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mОшибка парсинга результата: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mStdout: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mresult\u001b[49m.stdout\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mStderr: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult.stderr\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    274\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, feedback, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    275\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m4 - end gateway\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'result' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "for iteration in range(1, max_iterations + 1):\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Итерация {iteration}/{max_iterations}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    # Генерируем код\n",
    "    print(\"Генерация кода...\")\n",
    "    code = generate_code(framework, client, iteration, feedback, FINAL_DATA)\n",
    "    print(f\"Сгенерированный код:\\n{code}\\n\")\n",
    "    \n",
    "    # Тестируем код\n",
    "    print(\"Тестирование кода...\")\n",
    "    tests_passed, test_feedback, predict_path = evaluate_code(\n",
    "        code, framework, CSV_PATH, OUTPUT_DIR, iteration, FINAL_DATA\n",
    "    )\n",
    "    \n",
    "    print(f\"Тесты пройдены: {tests_passed}\")\n",
    "    print(f\"Обратная связь: {test_feedback}\")\n",
    "    if predict_path:\n",
    "        print(f\"Путь к предиктам: {predict_path}\")\n",
    "        # Загружаем и показываем предикты\n",
    "        try:\n",
    "            pred_df = pd.read_csv(predict_path)\n",
    "            print(f\"\\nПервые 5 строк предиктов:\")\n",
    "            print(pred_df.head())\n",
    "        except Exception as e:\n",
    "            print(f\"Ошибка загрузки предиктов: {e}\")\n",
    "    \n",
    "    print(\"\\n\")\n",
    "    \n",
    "    if tests_passed:\n",
    "        print(\"✅ Все тесты пройдены! Останавливаем итерации.\")\n",
    "        break\n",
    "    else:\n",
    "        feedback = test_feedback\n",
    "        print(\"❌ Тесты не пройдены. Переходим к следующей итерации с учетом обратной связи.\\n\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"Итоговый результат:\")\n",
    "print(f\"Итераций выполнено: {iteration}\")\n",
    "print(f\"Тесты пройдены: {tests_passed}\")\n",
    "print(f\"Путь к предиктам: {predict_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77cc4c40",
   "metadata": {},
   "source": [
    "## Просмотр сохраненных предиктов\n",
    "\n",
    "Если предикты были успешно сохранены, можно загрузить и проанализировать их\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d1cb4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Просмотр сохраненных предиктов\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "if predict_path and Path(predict_path).exists():\n",
    "    pred_df = pd.read_csv(predict_path)\n",
    "    print(f\"Загружено предиктов: {len(pred_df)}\")\n",
    "    print(f\"\\nПервые 10 строк:\")\n",
    "    print(pred_df.head(10))\n",
    "    print(f\"\\nСтатистика:\")\n",
    "    print(pred_df.describe())\n",
    "    \n",
    "    # Простая метрика точности\n",
    "    if 'true_label' in pred_df.columns and 'prediction' in pred_df.columns:\n",
    "        from sklearn.metrics import accuracy_score\n",
    "        accuracy = accuracy_score(pred_df['true_label'], pred_df['prediction'])\n",
    "        print(f\"\\nТочность (accuracy): {accuracy:.4f}\")\n",
    "else:\n",
    "    print(\"Предикты не найдены. Запустите предыдущую ячейку для генерации.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2916ba0",
   "metadata": {},
   "source": [
    "# Кодогенерация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8971f49e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Any\n",
    "def generate_code(\n",
    "    framework: str, llm: LLMClient, iteration: int, feedback: str, final_data: Dict[str, Any] | None = None\n",
    ") -> str:\n",
    "    \"\"\"Генерирует код для обучения модели.\"\"\"\n",
    "    preprocessing_info = \"\"\n",
    "    if final_data:\n",
    "        preprocessing_recipe = final_data.get(\"preprocessing_recipe\", {})\n",
    "        if preprocessing_recipe:\n",
    "            preprocessing_info = (\n",
    "                f\"\\n\\nИнформация о препроцессинге:\\n\"\n",
    "                f\"- Числовые колонки: {preprocessing_recipe.get('numeric_columns', [])}\\n\"\n",
    "                f\"- Категориальные колонки: {preprocessing_recipe.get('categorical_columns', [])}\\n\"\n",
    "                f\"- Тип задачи: {preprocessing_recipe.get('task_type', 'classification')}\\n\"\n",
    "            )\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "    Итерация {iteration}.\n",
    "    Ты — опытный Data Scientist. Нужно написать корректный код,\n",
    "    который использует МОЙ AutoML модуль (он уже импортирован; НЕ нужно определять класс AutoML).\n",
    "    Действуй так, будто class_labels и label определен и инициализирован!\n",
    "    \n",
    "    Код должен:\n",
    "    1) Использовать:\n",
    "        - df : pandas.DataFrame\n",
    "\n",
    "    2) Правильно разделять данные:\n",
    "        X =  df.drop(columns=[label])\n",
    "        y =  df[label]\n",
    "        где - label : str (метка класса)\n",
    "\n",
    "    3) Написать этот код для labels:\n",
    "        label_mapping = {{v: k for k, v in enumerate(class_labels)}}\n",
    "        if isinstance(y, pd.Series):\n",
    "            y = np.array(y.map(label_mapping), dtype=int)\n",
    "        else:\n",
    "            # Если y уже массив или другой тип, преобразуем через pandas для совместимости\n",
    "            y_series = pd.Series(y)\n",
    "            y = np.array(y_series.map(label_mapping), dtype=int)\n",
    "\n",
    "\n",
    "    4) Инициализировать AutoML с параметрами, например:\n",
    "        automl = AutoML(\n",
    "            task='classification',\n",
    "            use_preprocessing_pipeline=False,\n",
    "            feature_selector_type=None,\n",
    "            use_val_test_pipeline=False,\n",
    "            auto_models_init_kwargs={{ \n",
    "                \"metric\": \"roc_auc\",\n",
    "                \"time_series\": False,\n",
    "                \"models_list\": [\"linear\", \"forests\", \"boostings\"],\n",
    "                \"blend\": True,\n",
    "                \"stack\": True,\n",
    "                \"n_splits\": 10\n",
    "            }},\n",
    "            n_jobs=3,\n",
    "            random_state=0,\n",
    "        )\n",
    "\n",
    "    Описание параметров класса AutoML:\n",
    "     Parameters\n",
    "    ----------\n",
    "    task\n",
    "        Machine Learning task to solve, by default classification\n",
    "        Should be one of:\n",
    "            -\"classification (binary and multiclass)\n",
    "            - regression\n",
    "    use_preprocessing_pipeline, optional\n",
    "        Whether to use preprocessing pipeline, by default True\n",
    "    preprocessing_pipeline_kwargs, optional\n",
    "        Keyword arguments to initialize preprocessing_pipeline, by default {{ }}\n",
    "        List of possible arguments and their default values:\n",
    "            - pipe_steps = ['all']\n",
    "            - nan_share_ts=0.2\n",
    "            - qconst_feature_val_share_ts=0.95\n",
    "            - impute_num_strategy='median'\n",
    "            - impute_cat_strategy='most_frequent'\n",
    "            - outlier_capping_method='gaussian'\n",
    "            - outlier_cap_tail='both'\n",
    "            - corr_ts = 0.8\n",
    "            - corr_coef_methods=['pearson', 'spearman']\n",
    "            - corr_selection_method=\"missing_values\"\n",
    "            - oe_min_freq=0.1\n",
    "            - obj_encoders = ['oe', 'ohe']\n",
    "            - num_encoder = \"ss\"\n",
    "            - verbose=True\n",
    "    use_val_test_pipeline, optional\n",
    "        Whether to use val_test_pipeline, by default True\n",
    "    val_test_pipeline_kwargs, optional\n",
    "        Keyword arguments to initialize val_test_pipeline, by default {{ }}\n",
    "        List of possible arguments and their default values:\n",
    "            - pipe_steps = ['all']\n",
    "            - split_col='is_test_for_val'\n",
    "            - psi_cut_off=0.5\n",
    "            - psi_threshold=0.2\n",
    "            - psi_bins=15\n",
    "            - psi_strategy='equal_width'\n",
    "            - adversarial_auc_trshld=0.7\n",
    "            - verbose=True\n",
    "    feature_selector_type, optional\n",
    "        Whether to use feature_selector and which feature_selector to use, by default \"CatboostByShap\"\n",
    "        Possible values:\n",
    "            - CatboostByShap\n",
    "            - LAMA\n",
    "    feature_selector_kwargs, optional\n",
    "        Keyword arguments to initialize feature_selector, by default {{ }}\n",
    "        List of possible arguments and their default values:\n",
    "            * CatboostByShap\n",
    "                - n_features_to_select = 50\n",
    "                - complexity = \"Regular\"\n",
    "                - steps = 5\n",
    "            * LAMA\n",
    "                -task_type\n",
    "                - target_colname\n",
    "                - metric_name\n",
    "                - metric_direction\n",
    "                - timeout=120\n",
    "                - model='lama'\n",
    "                - strategy='RFA'\n",
    "                - permutation_n_repeats = 5\n",
    "    auto_models_init_kwargs, optional\n",
    "        Keyword arguments to initialize AutoModel, by default {{ }}\n",
    "        List of possible arguments and their default values:\n",
    "            - metric\n",
    "            - time_series=False\n",
    "            - models_list=None\n",
    "            - blend=False\n",
    "            - stack=False\n",
    "            - timeout=60 (timeout для моделей, которые его поддерживают, например TabularLama)\n",
    "    n_jobs, optional\n",
    "        Number of cores for parallel computations, by default 1\n",
    "    random_state, optional\n",
    "        Random state, by default 42\n",
    "    log_to_file, optional\n",
    "        Whether to save logs in files.\n",
    "        Save files locations:\n",
    "            - ml_data/YYYY_mm_dd___HH-MM-SS/logs.log for info logs\n",
    "            - ml_data/YYYY_mm_dd___HH-MM-SS/error.log for error, critical, warning logs\n",
    "\n",
    "    5) Обучить модель:\n",
    "        automl = automl.fit(\n",
    "            X, y,\n",
    "            auto_model_fit_kwargs={{\"tuning_timeout\": 10}}\n",
    "        )\n",
    "\n",
    "        — НЕ использовать sklearn в любом виде\n",
    "        — НЕ выполнять вручную препроцессинг, кодирование, скейлинг\n",
    "\n",
    "    6) Получить предсказания:\n",
    "        preds = automl.predict(X)\n",
    "\n",
    "    7) Вычислить переменные:\n",
    "        score = automl.auto_model.best_score\n",
    "        test_predictions = preds[:, 1]\n",
    "\n",
    "\n",
    "    Если передано preprocessing_recipe — учитывай его только как информационный блок,\n",
    "    но НЕ применяй его в коде.\n",
    "\n",
    "    Информация о препроцессинге:\n",
    "    {preprocessing_info}\n",
    "\n",
    "    ПРИМЕР КАК ИСПОЛЬЗУЕТСЯ МОЙ AutoML (НЕ копировать, только ориентир):\n",
    "        label_mapping = {{v: k for k, v in enumerate(class_labels)}}\n",
    "        if isinstance(y, pd.Series):\n",
    "            y = np.array(y.map(label_mapping), dtype=int)\n",
    "        else:\n",
    "            # Если y уже массив или другой тип, преобразуем через pandas для совместимости\n",
    "            y_series = pd.Series(y)\n",
    "            y = np.array(y_series.map(label_mapping), dtype=int)\n",
    "\n",
    "        automl = AutoML(...)\n",
    "        automl = automl.fit(X, y, auto_model_fit_kwargs={{\"tuning_timeout\": 10}})\n",
    "        score = automl.auto_model.best_score\n",
    "        test_predictions = automl.predict(X)[:, 1]\n",
    "\n",
    "\n",
    "\n",
    "    Требования к выводу:\n",
    "    - Вернуть СТРОГО: только код функции + необходимые импорты.\n",
    "    - Никакого текста, комментариев, markdown.\n",
    "    - Переменные: score и test_predictions — обязательны.\n",
    "    - Код я буду исполнять - НЕ ПИШИ ФУНКЦИЮ.\n",
    "    - Действуй так, будто class_labels и label определен и инициализирован!\n",
    "    \n",
    "\n",
    "    Обратная связь:\n",
    "    {feedback or \"нет\"}\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    fallback_code = _fallback_code_template_sklearn(final_data)\n",
    "    raw_code = llm.chat(prompt, fallback=fallback_code)\n",
    "    return _extract_code(raw_code) or fallback_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6a0dd265",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _fallback_code_template_sklearn(final_data: Dict[str, Any] | None = None) -> str:\n",
    "    \"\"\"Шаблон кода на scikit-learn для fallback.\"\"\"\n",
    "    numeric_cols = [\"duration\", \"credit_amount\", \"installment_commitment\", \"residence_since\", \"age\", \"existing_credits\", \"num_dependents\"]\n",
    "    categorical_cols = [\"checking_status\", \"credit_history\", \"purpose\", \"savings_status\", \"employment\", \"personal_status\", \"other_parties\", \"property_magnitude\", \"other_payment_plans\", \"housing\", \"job\", \"own_telephone\", \"foreign_worker\"]\n",
    "    \n",
    "    if final_data:\n",
    "        preprocessing_recipe = final_data.get(\"preprocessing_recipe\", {})\n",
    "        if preprocessing_recipe:\n",
    "            numeric_cols = preprocessing_recipe.get(\"numeric_columns\", numeric_cols)\n",
    "            categorical_cols = preprocessing_recipe.get(\"categorical_columns\", categorical_cols)\n",
    "    \n",
    "    return (\n",
    "        \"import pandas as pd\\n\"\n",
    "        \"import numpy as np\\n\"\n",
    "        \"from sklearn.compose import ColumnTransformer\\n\"\n",
    "        \"from sklearn.pipeline import Pipeline\\n\"\n",
    "        \"from sklearn.preprocessing import StandardScaler, OneHotEncoder\\n\"\n",
    "        \"from sklearn.impute import SimpleImputer\\n\"\n",
    "        \"from sklearn.ensemble import RandomForestClassifier\\n\\n\"\n",
    "        f\"numeric_cols = {numeric_cols}\\n\"\n",
    "        f\"categorical_cols = {categorical_cols}\\n\\n\"\n",
    "        \"def train_model(train_df: pd.DataFrame, test_df: pd.DataFrame, label: str):\\n\"\n",
    "        \"    # Подготовка признаков\\n\"\n",
    "        \"    numeric_transformer = Pipeline(steps=[\\n\"\n",
    "        \"        ('imputer', SimpleImputer(strategy='median')),\\n\"\n",
    "        \"        ('scaler', StandardScaler())\\n\"\n",
    "        \"    ])\\n\"\n",
    "        \"    categorical_transformer = Pipeline(steps=[\\n\"\n",
    "        \"        ('imputer', SimpleImputer(strategy='most_frequent')),\\n\"\n",
    "        \"        ('encoder', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\\n\"\n",
    "        \"    ])\\n\"\n",
    "        \"    preprocessor = ColumnTransformer(\\n\"\n",
    "        \"        transformers=[\\n\"\n",
    "        \"            ('num', numeric_transformer, numeric_cols),\\n\"\n",
    "        \"            ('cat', categorical_transformer, categorical_cols)\\n\"\n",
    "        \"        ]\\n\"\n",
    "        \"    )\\n\"\n",
    "        \"    \\n\"\n",
    "        \"    # Подготовка данных\\n\"\n",
    "        \"    X_train = train_df.drop(columns=[label])\\n\"\n",
    "        \"    y_train = train_df[label]\\n\"\n",
    "        \"    \\n\"\n",
    "        \"    # Создание пайплайна\\n\"\n",
    "        \"    model = Pipeline(steps=[\\n\"\n",
    "        \"        ('preprocessor', preprocessor),\\n\"\n",
    "        \"        ('classifier', RandomForestClassifier(n_estimators=100, random_state=42, max_depth=10))\\n\"\n",
    "        \"    ])\\n\"\n",
    "        \"    \\n\"\n",
    "        \"    # Обучение\\n\"\n",
    "        \"    model.fit(X_train, y_train)\\n\"\n",
    "        \"    \\n\"\n",
    "        \"    return model\\n\"\n",
    "    )\n",
    "\n",
    "\n",
    "def _extract_code(raw_output: str) -> str:\n",
    "    if \"```\" not in raw_output:\n",
    "        return raw_output.strip()\n",
    "    parts = raw_output.split(\"```\")\n",
    "    if len(parts) < 3:\n",
    "        return raw_output.strip()\n",
    "    code_block = parts[1]\n",
    "    if code_block.startswith((\"python\", \"py\")):\n",
    "        code_block = code_block.split(\"\\n\", 1)[1]\n",
    "    return code_block.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e82433c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "iteration = 1\n",
    "code = generate_code(framework, client, iteration, feedback, FINAL_DATA)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1d8833bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'import pandas as pd\\nimport numpy as np\\n\\nlabel_mapping = {v: k for k, v in enumerate(class_labels)}\\nif isinstance(y, pd.Series):\\n    y = np.array(y.map(label_mapping), dtype=int)\\nelse:\\n    y_series = pd.Series(y)\\n    y = np.array(y_series.map(label_mapping), dtype=int)\\n\\nX = df.drop(columns=[label])\\ny = df[label]\\n\\nautoml = AutoML(\\n    task=\\'classification\\',\\n    use_preprocessing_pipeline=False,\\n    feature_selector_type=None,\\n    use_val_test_pipeline=False,\\n    auto_models_init_kwargs={ \\n        \"metric\": \"roc_auc\",\\n        \"time_series\": False,\\n        \"models_list\": [\"linear\", \"forests\", \"boostings\"],\\n        \"blend\": True,\\n        \"stack\": True,\\n        \"n_splits\": 10\\n    },\\n    n_jobs=3,\\n    random_state=0,\\n)\\n\\nautoml = automl.fit(\\n    X, y,\\n    auto_model_fit_kwargs={\"tuning_timeout\": 10}\\n)\\n\\nscore = automl.auto_model.best_score\\npreds = automl.predict(df)\\ntest_predictions = preds[:, 1]'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c02b4ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "import sys\n",
    "from dataclasses import dataclass\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, Tuple\n",
    "\n",
    "if __package__ is None or __package__ == \"\":\n",
    "    from mas_automl.code_agent.load_mocks import load_mock_inputs  # type: ignore\n",
    "    from mas_automl.code_agent.openai_wraper import LLMClient, LLMConfig  # type: ignore\n",
    "    from mas_automl.code_agent.execnet_gateway import PythonSandboxClient, SandboxResult  # type: ignore\n",
    "else:\n",
    "    from .load_mocks import load_mock_inputs\n",
    "    from .openai_wraper import LLMClient, LLMConfig\n",
    "    from .execnet_gateway import PythonSandboxClient, SandboxResult\n",
    "\n",
    "DEFAULT_MAX_ITERATIONS = 3\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "CSV_PATH_TO_PREDICT = f\"C:\\\\Users\\\\User1\\\\Desktop\\\\ITMO_bootcamp\\\\data\\\\datasets\\\\TEST\\\\test_{timestamp}.csv\"\n",
    "\n",
    "@dataclass\n",
    "class PipelineResult:\n",
    "    framework: str\n",
    "    reason: str\n",
    "    code: str\n",
    "    tests_passed: bool\n",
    "    iterations: int\n",
    "    feedback: str\n",
    "    predict_path: str | None = None\n",
    "\n",
    "\n",
    "def evaluate_code(\n",
    "    code: str,\n",
    "    framework: str,\n",
    "    csv_path: str,\n",
    "    CSV_PATH_TO_PREDICT: str,\n",
    "    output_dir: str,\n",
    "    iteration: int,\n",
    "    final_data: Dict[str, Any] | None,\n",
    "    class_labels,\n",
    ") :\n",
    "    \"\"\"\n",
    "    Выполняет код в песочнице, тестирует функцию train_model и сохраняет предикты.\n",
    "    Возвращает (tests_passed, feedback, predict_path).\n",
    "    \"\"\"\n",
    "    sandbox = PythonSandboxClient.get()\n",
    "    \n",
    "    # Получаем информацию о датасете\n",
    "    target_column = \"class\"\n",
    "    if final_data:\n",
    "        target_column = final_data.get(\"manifest\", {}).get(\"target_column\", \"class\")\n",
    "    print(\"1 - start testing\")\n",
    "    # Подготавливаем код для выполнения\n",
    "    test_code = f\"\"\"\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import json\n",
    "from datetime import datetime\n",
    "from automl import AutoML\n",
    "\n",
    "class_labels = {class_labels}\n",
    "# Загружаем данные\n",
    "df = pd.read_csv(r'{csv_path}')\n",
    "print(f\"Загружено строк: {{len(df)}}\")\n",
    "label = 'class'\n",
    "# Генерированный код пользователя\n",
    "{code}\n",
    "\n",
    "# Выполняем функцию\n",
    "try:\n",
    "    df[\"test_predittions\"] = test_predictions\n",
    "    df.to_csv(r'{CSV_PATH_TO_PREDICT}', index=False)\n",
    "    # print(f\"Предикты сохранены в: {CSV_PATH_TO_PREDICT}\")\n",
    "    CSV_PATH_TO_PREDICT = r'{CSV_PATH_TO_PREDICT}'\n",
    "    # Простые проверки\n",
    "    errors = []\n",
    "   \n",
    "    if errors and any(errors):\n",
    "        raise ValueError(\"; \".join([e for e in errors if e]))\n",
    "    \n",
    "    result = {{\"ok\": True, \"predict_path\": CSV_PATH_TO_PREDICT, \"score\": score, \"message\": \"Все проверки пройдены\"}}\n",
    "    \n",
    "except Exception as e:\n",
    "    import traceback\n",
    "    result = {{\"ok\": False, \"predict_path\": None, \"score\": score, \"message\": str(e), \"traceback\": traceback.format_exc()}}\n",
    "\n",
    "# Выводим результат в формате JSON для парсинга\n",
    "print(\"RESULT_START\")\n",
    "print(json.dumps(result, ensure_ascii=False))\n",
    "print(\"RESULT_END\")\n",
    "\"\"\"\n",
    "    \n",
    "    # Выполняем код в песочнице\n",
    "    result = sandbox.run(test_code)\n",
    "    print(\"2 - end testing\")\n",
    "  \n",
    "    if not result.ok:\n",
    "        feedback = f\"Ошибка выполнения: {result.stderr}\\n{result.stdout}\"\n",
    "        return False, feedback, None, None\n",
    "    print(\"3 - try gateway\")\n",
    "\n",
    "    # Парсим результат из stdout\n",
    "    try:\n",
    "        stdout = result.stdout\n",
    "        # Ищем маркеры RESULT_START и RESULT_END\n",
    "        if \"RESULT_START\" in stdout and \"RESULT_END\" in stdout:\n",
    "            start_idx = stdout.find(\"RESULT_START\") + len(\"RESULT_START\")\n",
    "            end_idx = stdout.find(\"RESULT_END\")\n",
    "            result_json = stdout[start_idx:end_idx].strip()\n",
    "            result_dict = json.loads(result_json)\n",
    "        else:\n",
    "            # Fallback: пытаемся найти JSON в stdout\n",
    "            import re\n",
    "            json_match = re.search(r'\\{[^{}]*\"ok\"[^{}]*\\}', stdout)\n",
    "            if json_match:\n",
    "                result_dict = json.loads(json_match.group())\n",
    "            else:\n",
    "                result_dict = {\"ok\": False, \"message\": \"Не удалось найти результат в stdout\"}\n",
    "    except Exception as e:\n",
    "        feedback = f\"Ошибка парсинга результата: {e}\\nStdout: {result.stdout}\\nStderr: {result.stderr}\"\n",
    "        return False, feedback, None, None\n",
    "    print(\"4 - end gateway\")\n",
    "\n",
    "    if result_dict.get(\"ok\", False):\n",
    "        predict_path = result_dict.get(\"predict_path\")\n",
    "        score = result_dict.get(\"score\")\n",
    "        message = result_dict.get(\"message\", \"Проверки пройдены\")\n",
    "        return True, message, predict_path, score\n",
    "    else:\n",
    "        error_msg = result_dict.get(\"message\", \"Неизвестная ошибка\")\n",
    "        traceback_info = result_dict.get(\"traceback\", \"\")\n",
    "        score = result_dict.get(\"score\")\n",
    "\n",
    "        feedback = f\"Тесты не пройдены: {error_msg}\\n{traceback_info}\"\n",
    "        return False, feedback, None, None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ed49a52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Тестирование кода...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'code' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# Тестируем код\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mТестирование кода...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     10\u001b[39m tests_passed, test_feedback, predict_path, score = evaluate_code(\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m     \u001b[43mcode\u001b[49m, framework, CSV_PATH, CSV_PATH_TO_PREDICT, OUTPUT_DIR, iteration, FINAL_DATA,\n\u001b[32m     12\u001b[39m     class_labels=\u001b[38;5;28mlist\u001b[39m[Any](FINAL_DATA[\u001b[33m'\u001b[39m\u001b[33mmetafeatures\u001b[39m\u001b[33m'\u001b[39m][\u001b[33m'\u001b[39m\u001b[33mdataset\u001b[39m\u001b[33m'\u001b[39m][\u001b[33m'\u001b[39m\u001b[33mclass_balance\u001b[39m\u001b[33m'\u001b[39m].keys())\n\u001b[32m     13\u001b[39m )\n\u001b[32m     15\u001b[39m predict_path = CSV_PATH_TO_PREDICT\n\u001b[32m     17\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mТесты пройдены: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtests_passed\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'code' is not defined"
     ]
    }
   ],
   "source": [
    "# print(\"Генерация кода...\")\n",
    "# code = generate_code(framework, client, iteration, feedback, FINAL_DATA)\n",
    "# print(f\"Сгенерированный код:\\n{code}\\n\")\n",
    "from typing import Any\n",
    "\n",
    "\n",
    "iteration = 1 \n",
    "# Тестируем код\n",
    "print(\"Тестирование кода...\")\n",
    "tests_passed, test_feedback, predict_path, score = evaluate_code(\n",
    "    code, framework, CSV_PATH, CSV_PATH_TO_PREDICT, OUTPUT_DIR, iteration, FINAL_DATA,\n",
    "    class_labels=list[Any](FINAL_DATA['metafeatures']['dataset']['class_balance'].keys())\n",
    ")\n",
    "\n",
    "predict_path = CSV_PATH_TO_PREDICT\n",
    "\n",
    "print(f\"Тесты пройдены: {tests_passed}\")\n",
    "print(f\"Обратная связь: {test_feedback}\")\n",
    "if predict_path:\n",
    "    print(f\"Путь к предиктам: {predict_path}\")\n",
    "    # Загружаем и показываем предикты\n",
    "    try:\n",
    "        pred_df = pd.read_csv(predict_path)\n",
    "        print(f\"\\nПервые 5 строк предиктов:\")\n",
    "        print(pred_df.head())\n",
    "    except Exception as e:\n",
    "        print(f\"Ошибка загрузки предиктов: {e}\")\n",
    "\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2e5bdf19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(False,\n",
       " 'Тесты не пройдены: [Errno 22] Invalid argument: \\'C:\\\\\\\\Users\\\\\\\\User1\\\\\\\\Desktop\\\\\\\\ITMO_bootcamp\\\\\\\\data\\\\\\\\datasets\\\\\\\\TEST\\\\\\\\test_2025-11-15 12:31:26.017693.csv\\'\\nTraceback (most recent call last):\\n  File \"<string>\", line 57, in <module>\\n  File \"c:\\\\Users\\\\User1\\\\Desktop\\\\ITMO_bootcamp\\\\.venv\\\\Lib\\\\site-packages\\\\pandas\\\\util\\\\_decorators.py\", line 333, in wrapper\\n    return func(*args, **kwargs)\\n           ^^^^^^^^^^^^^^^^^^^^^\\n  File \"c:\\\\Users\\\\User1\\\\Desktop\\\\ITMO_bootcamp\\\\.venv\\\\Lib\\\\site-packages\\\\pandas\\\\core\\\\generic.py\", line 3989, in to_csv\\n    return DataFrameRenderer(formatter).to_csv(\\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \"c:\\\\Users\\\\User1\\\\Desktop\\\\ITMO_bootcamp\\\\.venv\\\\Lib\\\\site-packages\\\\pandas\\\\io\\\\formats\\\\format.py\", line 1014, in to_csv\\n    csv_formatter.save()\\n  File \"c:\\\\Users\\\\User1\\\\Desktop\\\\ITMO_bootcamp\\\\.venv\\\\Lib\\\\site-packages\\\\pandas\\\\io\\\\formats\\\\csvs.py\", line 251, in save\\n    with get_handle(\\n         ^^^^^^^^^^^\\n  File \"c:\\\\Users\\\\User1\\\\Desktop\\\\ITMO_bootcamp\\\\.venv\\\\Lib\\\\site-packages\\\\pandas\\\\io\\\\common.py\", line 873, in get_handle\\n    handle = open(\\n             ^^^^^\\nOSError: [Errno 22] Invalid argument: \\'C:\\\\\\\\Users\\\\\\\\User1\\\\\\\\Desktop\\\\\\\\ITMO_bootcamp\\\\\\\\data\\\\\\\\datasets\\\\\\\\TEST\\\\\\\\test_2025-11-15 12:31:26.017693.csv\\'\\n',\n",
       " 'C:\\\\Users\\\\User1\\\\Desktop\\\\ITMO_bootcamp\\\\data\\\\datasets\\\\TEST\\\\test_2025-11-15 12:31:26.017693.csv')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tests_passed, test_feedback, predict_path "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8dc395a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(False,\n",
       " 'Тесты не пройдены: \\'numpy.ndarray\\' object has no attribute \\'to_csv\\'\\nTraceback (most recent call last):\\n  File \"<string>\", line 57, in <module>\\nAttributeError: \\'numpy.ndarray\\' object has no attribute \\'to_csv\\'\\n',\n",
       " 'C:\\\\Users\\\\User1\\\\Desktop\\\\ITMO_bootcamp\\\\data\\\\datasets\\\\TEST\\\\test1__.csv')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tests_passed, test_feedback, predict_path "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f59277b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "### Selected Framework\n",
       "**Framework:** H2O AutoML\n",
       "\n",
       "### Reasoning\n",
       "H2O AutoML является наиболее стабильным и универсальным выбором для табличных данных данного размера (1000 строк, 21 признаков) и типа задачи (бинарная классификация). Оно предлагает надежный стекинг моделей (GBM, XGBoost, GLM, DRF, DeepLearning), автоматическую кросс-валидацию, раннюю остановку и поддержку production-сценариев. Несмотря на отсутствие поддержки мультимодальности и GPU для всех моделей, оно хорошо подходит для задач классификации с высокой воспроизводимостью и стабильностью, что особенно важно в корпоративных и промышленных условиях. В отличие от AutoGluon, которое требует больше ресурсов и может быть менее предсказуемо на небольших наборах данных, H2O AutoML обеспечивает более предсказуемое поведение и более широкую совместимость с существующими инфраструктурными решениями.\n",
       "\n",
       "### Generated Prompt\n",
       "input_variables=['analysis_json', 'frameworks_list', 'metadata_json'] input_types={} partial_variables={} messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='Ты — эксперт по AutoML и ML-инженер. Твоя задача — выбрать наиболее подходящий AutoML-фреймворк для табличных данных. Оцени качество данных, размер, тип задачи, ограничения и предложи лучший вариант из списка.'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['analysis_json', 'frameworks_list', 'metadata_json'], input_types={}, partial_variables={}, template='Вот анализ датасета и метаданные:\\n\\n### 📊 Data Analysis\\n{analysis_json}\\n\\n### 🧾 Metadata\\n{metadata_json}\\n\\n### ⚙️ Доступные AutoML фреймворки\\n{frameworks_list}\\n\\nПоясни свой выбор кратко, но содержательно. Если несколько подходят, выбери наиболее универсальный и стабильный вариант. Ответ верни в формате JSON:\\n{{\"framework\": \"...\", \"reason\": \"...\"}}'), additional_kwargs={})]\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "# Your existing call (unchanged)\n",
    "from mas_automl.code_agent.base_pipeline import choose_framework\n",
    "\n",
    "framework, reason, prompt = choose_framework(\n",
    "    DATA_ANALYZE, METADATA, REGISTRY, FINAL_DATA, client)\n",
    "\n",
    "# Pretty display in notebook\n",
    "display(Markdown(f\"\"\"\n",
    "### Selected Framework\n",
    "**Framework:** {framework}\n",
    "\n",
    "### Reasoning\n",
    "{reason}\n",
    "\n",
    "### Generated Prompt\n",
    "{prompt}\n",
    "\"\"\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "efb6d4f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.prompts.chat.ChatPromptTemplate'>\n"
     ]
    }
   ],
   "source": [
    "print(type(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e9ce2a",
   "metadata": {},
   "source": [
    "# Кодогенерация"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
