from __future__ import annotations

import json
import sys
from dataclasses import dataclass
from pathlib import Path
from typing import Any, Dict, Tuple

if __package__ is None or __package__ == "":
    sys.path.append(str(Path(__file__).resolve().parents[2]))
    from mas_automl.code_agent.load_mocks import load_mock_inputs  # type: ignore
    from mas_automl.code_agent.openai_wraper import LLMClient, LLMConfig  # type: ignore
else:
    from .load_mocks import load_mock_inputs
    from .openai_wraper import LLMClient, LLMConfig

DEFAULT_MAX_ITERATIONS = 3

from langchain_core.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate


@dataclass
class PipelineResult:
    framework: str
    reason: str
    code: str
    tests_passed: bool
    iterations: int
    feedback: str


def run_pipeline(max_iterations: int = DEFAULT_MAX_ITERATIONS, llm: LLMClient | None = None) -> PipelineResult:
    """ĞÑĞ½Ğ¾Ğ²Ğ½Ğ¾Ğ¹ ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸Ğ¹: Ğ·Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ° Ğ¼Ğ¾ĞºĞ¾Ğ² â†’ Ğ²Ñ‹Ğ±Ğ¾Ñ€ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€ĞºĞ° â†’ ĞºĞ¾Ğ´Ğ¾Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ â†’ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ°."""
    data_analysis, metadata, registry = load_mock_inputs()
    llm = llm or LLMClient(LLMConfig())

    framework, reason = choose_framework(data_analysis, metadata, registry, llm)

    feedback = ""
    code = ""
    tests_passed = False
    iteration = 0

    for iteration in range(1, max_iterations + 1):
        code = generate_code(framework, llm, iteration, feedback)
        tests_passed, feedback = evaluate_code(code, framework)
        if tests_passed:
            break

    return PipelineResult(
        framework=framework,
        reason=reason,
        code=code,
        tests_passed=tests_passed,
        iterations=iteration,
        feedback=feedback,
    )


def choose_framework(
    data_analysis: Dict[str, Any],
    metadata: Dict[str, Any],
    registry: Dict[str, str],
    llm: LLMClient,
) -> Tuple[str, str]:
    """
    Ğ Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ²Ñ‹Ğ±Ğ¾Ñ€ Ğ»ÑƒÑ‡ÑˆĞµĞ³Ğ¾ AutoML-Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€ĞºĞ° Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ChatPromptTemplate.
    """

    # --- Ğ”ĞµÑ„Ğ¾Ğ»Ñ‚Ğ½Ñ‹Ğµ Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ñ Ğ¸ fallback ---
    fallback_framework = _fallback_framework_choice(metadata, registry)
    fallback = json.dumps(
        {"framework": fallback_framework, "reason": "Ğ­Ğ²Ñ€Ğ¸ÑÑ‚Ğ¸ĞºĞ° Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ‚Ğ¸Ğ¿Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¸ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ° Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºĞ¸."},
        ensure_ascii=False,
    )

    # --- Ğ¤Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ¸Ñ€ÑƒĞµĞ¼ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ´Ğ»Ñ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ° ---
    frameworks_list = "\n".join(
                            f"\n\n###### {name} ---> {desc} ######"
                            for name, desc in registry.items()
    )

    metadata_json = json.dumps(metadata, ensure_ascii=False, indent=2)
    analysis_json = json.dumps(data_analysis, ensure_ascii=False, indent=2)

    # --- Prompt ÑˆĞ°Ğ±Ğ»Ğ¾Ğ½ ---
    prompt = ChatPromptTemplate.from_messages([
        SystemMessagePromptTemplate.from_template(
            "Ğ¢Ñ‹ â€” ÑĞºÑĞ¿ĞµÑ€Ñ‚ Ğ¿Ğ¾ AutoML Ğ¸ ML-Ğ¸Ğ½Ğ¶ĞµĞ½ĞµÑ€. "
            "Ğ¢Ğ²Ğ¾Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ° â€” Ğ²Ñ‹Ğ±Ñ€Ğ°Ñ‚ÑŒ Ğ½Ğ°Ğ¸Ğ±Ğ¾Ğ»ĞµĞµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ÑÑ‰Ğ¸Ğ¹ AutoML-Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. "
            "ĞÑ†ĞµĞ½Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ñ€Ğ°Ğ·Ğ¼ĞµÑ€, Ñ‚Ğ¸Ğ¿ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸, Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸ Ğ»ÑƒÑ‡ÑˆĞ¸Ğ¹ Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚ Ğ¸Ğ· ÑĞ¿Ğ¸ÑĞºĞ°."
        ),
        HumanMessagePromptTemplate.from_template(
            "Ğ’Ğ¾Ñ‚ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ° Ğ¸ Ğ¼ĞµÑ‚Ğ°Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ:\n\n"
            "### ğŸ“Š Data Analysis\n{analysis_json}\n\n"
            "### ğŸ§¾ Metadata\n{metadata_json}\n\n"
            "### âš™ï¸ Ğ”Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ñ‹Ğµ AutoML Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€ĞºĞ¸\n{frameworks_list}\n\n"
            "ĞŸĞ¾ÑÑĞ½Ğ¸ ÑĞ²Ğ¾Ğ¹ Ğ²Ñ‹Ğ±Ğ¾Ñ€ ĞºÑ€Ğ°Ñ‚ĞºĞ¾, Ğ½Ğ¾ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾. "
            "Ğ•ÑĞ»Ğ¸ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ÑÑ‚, Ğ²Ñ‹Ğ±ĞµÑ€Ğ¸ Ğ½Ğ°Ğ¸Ğ±Ğ¾Ğ»ĞµĞµ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¸ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚. "
            "ĞÑ‚Ğ²ĞµÑ‚ Ğ²ĞµÑ€Ğ½Ğ¸ Ğ² Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğµ JSON:\n"
            "{{\"framework\": \"...\", \"reason\": \"...\"}}"
        )
    ])
  
    # --- Ğ¤Ğ¾Ñ€Ğ¼Ğ¸Ñ€ÑƒĞµĞ¼ Ñ„Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ñ‚ĞµĞºÑÑ‚ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ° ---
    formatted_prompt = prompt.format_messages(
        analysis_json=analysis_json,
        metadata_json=metadata_json,
        frameworks_list=frameworks_list,
    )

    # --- Ğ’Ñ‹Ğ·Ğ¾Ğ² LLM ---
    try:
        llm_response = llm._client.invoke(formatted_prompt)
        content = getattr(llm_response, "content", "").strip()
        parsed = json.loads(content)
        framework = parsed.get("framework", fallback_framework)
        reason = parsed.get("reason", "Ğ‘ĞµĞ· Ğ¾Ğ±ÑŠÑÑĞ½ĞµĞ½Ğ¸Ñ.")
    except Exception as e:
        print("âš ï¸ ĞÑˆĞ¸Ğ±ĞºĞ° Ğ¿Ñ€Ğ¸ Ğ²Ñ‹Ğ·Ğ¾Ğ²Ğµ LLM Ğ¸Ğ»Ğ¸ Ğ¿Ğ°Ñ€ÑĞ¸Ğ½Ğ³Ğµ:", repr(e))
        framework, reason = fallback_framework, "ĞÑˆĞ¸Ğ±ĞºĞ° Ğ² Ğ¾Ñ‚Ğ²ĞµÑ‚Ğµ LLM; Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ° ÑĞ²Ñ€Ğ¸ÑÑ‚Ğ¸ĞºĞ°."

    # --- ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° Ğ²Ğ°Ğ»Ğ¸Ğ´Ğ½Ğ¾ÑÑ‚Ğ¸ ---
    if framework not in registry:
        framework = fallback_framework
        reason = f"LLM Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ» Ğ½ĞµĞ¸Ğ·Ğ²ĞµÑÑ‚Ğ½Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº. Ğ­Ğ²Ñ€Ğ¸ÑÑ‚Ğ¸ĞºĞ°: {reason}"

    return framework, reason, prompt


def _fallback_framework_choice(metadata: Dict[str, Any], registry: Dict[str, str]) -> str:
    dataset_type = (metadata.get("dataset_type") or "").lower()
    num_rows = metadata.get("num_rows") or 0

    if dataset_type in {"classification", "binary"} and num_rows <= 50_000 and "AutoGluon" in registry:
        return "AutoGluon"
    if num_rows > 100_000 and "H2O AutoML" in registry:
        return "H2O AutoML"
    if "LightAutoML" in registry:
        return "LightAutoML"
    return next(iter(registry.keys()))

def generate_code(framework: str, llm: LLMClient, iteration: int, feedback: str) -> str:
    prompt = (
        f"Ğ˜Ñ‚ĞµÑ€Ğ°Ñ†Ğ¸Ñ {iteration}. ĞĞ°Ğ¿Ğ¸ÑˆĞ¸ ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½ÑƒÑ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ train_model(...) Ğ´Ğ»Ñ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€ĞºĞ° {framework}. "
        "Ğ¤ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ´Ğ¾Ğ»Ğ¶Ğ½Ğ° Ğ¿Ñ€Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ñ‚ÑŒ train_df, test_df Ğ¸ label, Ğ²Ñ‹Ğ·Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¸ Ğ²Ğ¾Ğ·Ğ²Ñ€Ğ°Ñ‰Ğ°Ñ‚ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½ÑƒÑ ÑÑƒÑ‰Ğ½Ğ¾ÑÑ‚ÑŒ. "
        "Ğ•ÑĞ»Ğ¸ ĞµÑÑ‚ÑŒ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ°Ñ ÑĞ²ÑĞ·ÑŒ Ğ¾Ñ‚ Ñ‚ĞµÑÑ‚Ğ¾Ğ², ÑƒÑ‡Ñ‚Ğ¸ ĞµÑ‘.\n"
        f"ĞĞ±Ñ€Ğ°Ñ‚Ğ½Ğ°Ñ ÑĞ²ÑĞ·ÑŒ: {feedback or 'Ğ½ĞµÑ‚'}\n"
        "Ğ’ĞµÑ€Ğ½Ğ¸ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ ĞºĞ¾Ğ´ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ Ğ¸ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ñ‹Ğµ Ğ¸Ğ¼Ğ¿Ğ¾Ñ€Ñ‚Ñ‹ (Ğ±ĞµĞ· Ğ¿Ğ¾ÑÑĞ½ĞµĞ½Ğ¸Ğ¹)."
    )
    fallback_code = _fallback_code_template(framework)
    raw_code = llm.chat(prompt, fallback=fallback_code)
    return _extract_code(raw_code) or fallback_code


def evaluate_code(code: str, framework: str) -> Tuple[bool, str]:
    """ĞŸÑ€Ğ¾ÑÑ‚ĞµĞ¹ÑˆĞ¸Ğµ ÑĞ²Ñ€Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ¸ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ‚ĞµÑÑ‚Ğ¾Ğ²."""
    code_lower = code.lower()
    framework_token = framework.split()[0].lower()

    if framework_token not in code_lower:
        return False, f"Ğ’ ĞºĞ¾Ğ´Ğµ Ğ´Ğ¾Ğ»Ğ¶Ğ½Ğ¾ Ğ±Ñ‹Ñ‚ÑŒ ÑƒĞ¿Ğ¾Ğ¼Ğ¸Ğ½Ğ°Ğ½Ğ¸Ğµ {framework}."
    if "def train_model" not in code:
        return False, "ĞĞ¶Ğ¸Ğ´Ğ°ĞµÑ‚ÑÑ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ train_model(...)."
    if "fit" not in code_lower and "train" not in code_lower:
        return False, "ĞšĞ¾Ğ´ Ğ´Ğ¾Ğ»Ğ¶ĞµĞ½ Ğ²Ñ‹Ğ·Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ (fit/train)."

    return True, "ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ¸ Ğ¿Ñ€Ğ¾Ğ¹Ğ´ĞµĞ½Ñ‹."


def _fallback_code_template(framework: str) -> str:
    name = framework.lower()
    if "autogluon" in name:
        return (
            "import pandas as pd\n"
            "from autogluon.tabular import TabularPredictor\n\n"
            "def train_model(train_df: pd.DataFrame, test_df: pd.DataFrame, label: str) -> TabularPredictor:\n"
            "    predictor = TabularPredictor(label=label)\n"
            "    predictor.fit(train_data=train_df, presets='medium_quality', time_limit=60)\n"
            "    predictor.evaluate(test_df, silent=True)\n"
            "    return predictor\n"
        )
    if "lightautoml" in name:
        return (
            "import pandas as pd\n"
            "from lightautoml.automl.presets.tabular_presets import TabularAutoML\n"
            "from lightautoml.tasks import Task\n\n"
            "def train_model(train_df: pd.DataFrame, test_df: pd.DataFrame, label: str):\n"
            "    task = Task('binary')\n"
            "    automl = TabularAutoML(task=task, timeout=300)\n"
            "    automl.fit_predict(train_df, roles={'target': label})\n"
            "    return automl\n"
        )
    if "h2o" in name:
        return (
            "import h2o\n"
            "from h2o.automl import H2OAutoML\n\n"
            "def train_model(train_df, test_df, label):\n"
            "    h2o.init()\n"
            "    train_hf = h2o.H2OFrame(train_df)\n"
            "    aml = H2OAutoML(max_runtime_secs=120)\n"
            "    aml.train(y=label, training_frame=train_hf)\n"
            "    return aml\n"
        )
    return (
        "def train_model(train_df, test_df, label):\n"
        f"    raise NotImplementedError('ĞĞµĞ¸Ğ·Ğ²ĞµÑÑ‚Ğ½Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº: {framework}')\n"
    )


def _extract_code(raw_output: str) -> str:
    if "```" not in raw_output:
        return raw_output.strip()
    parts = raw_output.split("```")
    if len(parts) < 3:
        return raw_output.strip()
    code_block = parts[1]
    if code_block.startswith(("python", "py")):
        code_block = code_block.split("\n", 1)[1]
    return code_block.strip()


__all__ = [
    "PipelineResult",
    "run_pipeline",
    "choose_framework",
    "generate_code",
    "evaluate_code",
]


if __name__ == "__main__":
    result = run_pipeline()
    print(json.dumps(result.__dict__, ensure_ascii=False, indent=2))

